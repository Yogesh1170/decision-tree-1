{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579c4231",
   "metadata": {},
   "source": [
    "## Q1. Describe the decision tree classifier algorithm and how it works to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26675eb",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1e5fa",
   "metadata": {},
   "source": [
    "A Decision Tree classifier is a popular algorithm used in machine learning for both classification and regression tasks. It operates by creating a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "**Tree Structure:**\n",
    "\n",
    "    Nodes: Each internal node represents a \"test\" on an attribute (e.g., whether a person is taller than 5 feet).\n",
    "\n",
    "    Branches: Each branch represents the outcome of the test.\n",
    "\n",
    "    Leaves: Each leaf node represents a class label (classification) or a continuous value (regression).\n",
    "\n",
    "**Splitting:**\n",
    "\n",
    "    The process starts at the root node and splits the data on the feature that results in the most significant information gain (or another measure like Gini impurity).\n",
    "\n",
    "    This step is recursively applied to each child node, creating a subtree until one of the stopping criteria is met (e.g., maximum depth, minimum samples per leaf).\n",
    "\n",
    "**Making Predictions:**\n",
    "\n",
    "    To classify a new observation, the algorithm starts at the root and traverses down the tree according to the feature values of the observation.\n",
    "\n",
    "    The path taken is determined by the outcomes of the tests at each node until it reaches a leaf node.\n",
    "\n",
    "    The prediction for that observation is the label of the leaf node.\n",
    "\n",
    "**Key Concepts**\n",
    "\n",
    "    Information Gain: A measure of the effectiveness of an attribute in classifying the training data. The algorithm aims to maximize information gain at each split.\n",
    "\n",
    "    Gini Impurity: A measure of impurity or impurity of a sample, often used by the CART (Classification and Regression Tree) algorithm.\n",
    "\n",
    "    Entropy: Used in the ID3 algorithm, it's a measure of disorder or impurity in the data.\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "    Simple to understand and interpret.\n",
    "    Requires little data preprocessing.\n",
    "    Can handle both numerical and categorical data.\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "    Prone to overfitting, especially with deep trees.\n",
    "    Can be unstable with small changes in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372c3dfb",
   "metadata": {},
   "source": [
    "## Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5149c5aa",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25814d9",
   "metadata": {},
   "source": [
    "**Step 1:** Understanding the Basics\n",
    "Imagine we have a dataset with features (attributes) and a target variable (class label). Our goal is to create a model that can predict the target variable based on the features.\n",
    "\n",
    "**Step 2:** Starting at the Root\n",
    "We start with the entire dataset at the root of the tree. Our task is to decide which feature to split the data on first. To make this decision, we use a concept called Information Gain.\n",
    "\n",
    "**Step 3:** Information Gain\n",
    "Information Gain measures how well a feature separates the data into distinct classes. It's based on the concept of Entropy, which quantifies the amount of uncertainty or impurity in the dataset.\n",
    "\n",
    "***Entropy:*** Entropy ùêª(ùëÜ) of a dataset ùëÜ with two classes (positive and negative) is calculated as:\n",
    "\n",
    "$$ H(S) = -p_+ \\log_2(p_+) - p_- \\log_2(p_-)$$\n",
    "\n",
    "where, ùëù_{+} is the proportion of positive examples in ùëÜ, and ùëù_{‚àí} is the proportion of negative examples in ùëÜ.\n",
    "\n",
    "***Information Gain:*** Information Gain ùêºùê∫(ùëá,ùê¥) for a feature ùê¥ is the difference between the entropy of the dataset ùëá and the weighted entropy after splitting on feature ùê¥:\n",
    "\n",
    "$$ IG(T, A) = H(T) - \\sum_{v \\in Values(A)} \\frac{|T_v|}{|T|} H(T_V)$$\n",
    "\n",
    "Here, ùëá is the original dataset, ùëá_{ùë£} is the subset of ùëá where feature ùê¥ has value ùë£, and ùëâùëéùëôùë¢ùëíùë†(ùê¥) are the possible values of ùê¥.\n",
    "\n",
    "**Step 4:** Splitting the Data\n",
    "We calculate the Information Gain for each feature and choose the one with the highest gain. This feature becomes the root node of the tree. We then split the data based on this feature.\n",
    "\n",
    "**Step 5:** Recursion\n",
    "For each subset created by the split, we repeat the process. We treat each subset as a new dataset and find the best feature to split on, using Information Gain again. This process continues recursively until one of the stopping criteria is met:\n",
    "\n",
    "    All instances in a subset belong to the same class.\n",
    "    There are no more features to split.\n",
    "    A maximum tree depth is reached.\n",
    "\n",
    "**Step 6:** Making Predictions\n",
    "To make a prediction for a new instance, we start at the root of the tree and traverse down according to the values of the instance's features. The path we follow through the tree leads us to a leaf node, which provides the predicted class.\n",
    "\n",
    "This process creates a model that can classify new instances based on the decision rules derived from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c375a",
   "metadata": {},
   "source": [
    "## Q3. Explain how a decision tree classifier can be used to solve a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ba66c",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f0578",
   "metadata": {},
   "source": [
    "**Step 1: Initial Setup**\n",
    "\n",
    "Start with a dataset comprising multiple features and a target variable with two possible classes (e.g., positive and negative).\n",
    "\n",
    "**Step 2: Calculating Entropy**\n",
    "\n",
    "Calculate the entropy of the target variable in the entire dataset, which gives an indication of the impurity or uncertainty of the data.\n",
    "$$ H(S) = -p_+ \\log_2(p_+) - p_- \\log_2(p_-)$$\n",
    "\n",
    "where ùëù+ is the proportion of positive examples, and ùëù‚àí is the proportion of negative examples.\n",
    "\n",
    "**Step 3: Information Gain**\n",
    "\n",
    "Evaluate how each feature reduces the entropy and thereby improves the classification. Information gain is used to quantify this improvement.\n",
    "\n",
    "$$ IG(T, A) = H(T) - \\sum_{v \\in Values(A)} \\frac{|T_v|}{|T|} H(T_v) $$\n",
    "\n",
    "Here, ùëá represents the original dataset, ùê¥ is the feature being considered, ùëáùë£ is the subset of ùëá where the feature ùê¥ takes value ùë£, and ùëâùëéùëôùë¢ùëíùë†(ùê¥) are the possible values of ùê¥.\n",
    "\n",
    "**Step 4: Selecting the Best Feature**\n",
    "\n",
    "Choose the feature with the highest information gain as the decision node. This feature effectively splits the data into purer subsets.\n",
    "\n",
    "**Step 5: Recursively Building the Tree**\n",
    "\n",
    "Repeat the process for each subset, treating each as a new dataset, and selecting the best feature to split on. This recursive splitting continues until a stopping criterion is met, such as reaching a maximum tree depth or having all instances in a subset belong to the same class.\n",
    "\n",
    "**Step 6: Making Predictions**\n",
    "\n",
    "Once the tree is built, new instances are classified by traversing the tree from the root to a leaf, following the decision rules at each node. The class label at the leaf is the predicted class for the instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3bd77",
   "metadata": {},
   "source": [
    "## Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51233b3",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b82af",
   "metadata": {},
   "source": [
    "***Geometric Intuition***\n",
    "**Feature Space Division:**\n",
    "\n",
    "    A decision tree partitions the feature space into axis-aligned rectangles or hyperrectangles. Each split on a feature creates a new boundary that separates the data points based on that feature's value.\n",
    "\n",
    "**Hierarchical Splits:**\n",
    "\n",
    "    The decision tree is built in a hierarchical manner, with each internal node representing a split on a specific feature. These splits create a series of nested regions, where each region corresponds to a leaf node in the tree.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Imagine a 2-dimensional feature space with features ùë•1 and ùë•2:\n",
    "\n",
    "    The root node might split on ùë•1, creating two region: ùë•1‚â§ùë°1 and ùë•1>ùë°1.\n",
    "\n",
    "    Each of these regions can then be further split based on ùë•2 or ùë•1, creating smaller and smaller regions.\n",
    "\n",
    "***Making Predictions***\n",
    "**Traversing the Tree:**\n",
    "\n",
    "    To classify a new data point, the decision tree algorithm starts at the root node and traverses the tree based on the feature values of the data point.\n",
    "\n",
    "    At each node, a test on a feature is performed, directing the traversal to either the left or right child node based on the outcome of the test.\n",
    "\n",
    "**Reaching a Leaf Node:**\n",
    "\n",
    "    This process continues until a leaf node is reached. The leaf node contains the class label that is assigned as the prediction for the data point.\n",
    "\n",
    "**Visualization**\n",
    "\n",
    "    If visualized, the decision boundaries created by the tree can be seen as vertical and horizontal lines in the feature space, forming a grid-like structure.\n",
    "\n",
    "    Each cell in this grid corresponds to a leaf node and is associated with a specific class label.\n",
    "\n",
    "By dividing the feature space in this hierarchical and rectilinear manner, decision trees create intuitive and interpretable models that can be used to classify new data points based on the learned decision rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96c94d7",
   "metadata": {},
   "source": [
    "## Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15640715",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b5cee9",
   "metadata": {},
   "source": [
    "The confusion matrix is a table used to evaluate the performance of a classification model. It provides a detailed breakdown of the model's predictions against the actual outcomes, making it easier to understand how well the model is performing. Here's a breakdown of its components:\n",
    "\n",
    "**Confusion Matrix Components**\n",
    "For a binary classification problem, the confusion matrix has four key components:\n",
    "\n",
    "    True Positives (TP): The number of instances correctly classified as the positive class.\n",
    "\n",
    "    True Negatives (TN): The number of instances correctly classified as the negative class.\n",
    "\n",
    "    False Positives (FP): The number of instances incorrectly classified as the positive class (Type I error).\n",
    "\n",
    "    False Negatives (FN): The number of instances incorrectly classified as the negative class (Type II error).\n",
    "\n",
    "**Confusion Matrix Table**\n",
    "The confusion matrix can be represented in a table like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71585e",
   "metadata": {},
   "source": [
    "|                | Predicted Positive | Predicted Negative |\n",
    "|----------------|--------------------|--------------------|\n",
    "| **Actual Positive** | True Positive (TP)                | False Negative (FN)         | \n",
    "| **Actual Negative** | False Positive (FP)               | True Negative (TN)          | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91572fb4",
   "metadata": {},
   "source": [
    "**Using the Confusion Matrix to Evaluate Performance**\n",
    "The confusion matrix can be used to calculate various performance metrics that provide insights into different aspects of the model's performance:\n",
    "\n",
    "    Accuracy: The proportion of correct predictions (both true positives and true negatives) out of all predictions.\n",
    "    \n",
    "$$ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "    Precision: The proportion of true positive predictions out of all positive predictions (how many selected items are relevant).\n",
    "    \n",
    "$$ \\text{Precision} = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "    Recall (Sensitivity): The proportion of true positive predictions out of all actual positives (how many relevant items are selected).\n",
    "    \n",
    "$$ \\text{Recall} = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "    F1 Score: The harmonic mean of precision and recall, providing a single metric that balances both concerns.\n",
    "    \n",
    "$$ F1 \\text{ Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\n",
    "    \n",
    "    Specificity: The proportion of true negative predictions out of all actual negatives.\n",
    "    \n",
    "$$ \\text{Specificity} = \\frac{TN}{TN + FP} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2a0443",
   "metadata": {},
   "source": [
    "## Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be calculated from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b14fd",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb8c1be",
   "metadata": {},
   "source": [
    "Example Confusion Matrix\n",
    "Assume we have a binary classification problem where our model makes the following predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1431fc47",
   "metadata": {},
   "source": [
    "| Actual / Predicted | Positive | Negative |\n",
    "|--------------------|----------|----------|\n",
    "| **Positive**       |    50    |    10    |\n",
    "| **Negative**       |    5     |    35    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7a75fd",
   "metadata": {},
   "source": [
    "In this confusion matrix:\n",
    "\n",
    "    True Positives (TP): 50\n",
    "\n",
    "    False Positives (FP): 5\n",
    "\n",
    "    True Negatives (TN): 35\n",
    "\n",
    "    False Negatives (FN): 10\n",
    "\n",
    "**Calculating Performance Metrics**\n",
    "\n",
    "    Precision is the proportion of true positive predictions out of all positive predictions.\n",
    "    \n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{50}{50 + 5} = \\frac{50}{55} \\approx 0.909$$\n",
    "\n",
    "    Recall (Sensitivity) is the proportion of true positive predictions out of all actual positives.\n",
    "    \n",
    "$$ \\text{Recall} = \\frac{TP}{TP + FN} = \\frac{50}{50 + 10} = \\frac{50}{60} \\approx 0.833 $$\n",
    "\n",
    "    F1 Score is the harmonic mean of precision and recall, balancing both metrics.\n",
    "    \n",
    "$$ F1 \\text{ Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = 2 \\cdot \\frac{0.909 \\cdot 0.833}{0.909 + 0.833} \\approx 0.87 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d26fdf",
   "metadata": {},
   "source": [
    "## Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and explain how this can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428bd5c0",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a726b8ed",
   "metadata": {},
   "source": [
    "Choosing an appropriate evaluation metric for a classification problem is crucial because it directly affects how the model's performance is assessed and interpreted. Different metrics highlight different aspects of performance and can provide insights into various strengths and weaknesses of a model. Here's a discussion on the importance and how to choose the right metric:\n",
    "\n",
    "***Importance of Choosing the Right Evaluation Metric***\n",
    "**Context and Objectives:**\n",
    "\n",
    "    The choice of metric should align with the specific goals and context of the problem. For instance, in medical diagnostics, minimizing false negatives might be more critical than false positives, as missing a diagnosis could be more harmful than a false alarm.\n",
    "\n",
    "**Class Imbalance:**\n",
    "\n",
    "    In cases where classes are imbalanced (one class is much more frequent than the other), accuracy may not be a sufficient metric. A model could achieve high accuracy by simply predicting the majority class, neglecting the minority class. Metrics like precision, recall, and F1 score become more relevant.\n",
    "\n",
    "**Error Types:**\n",
    "\n",
    "    Different applications have different tolerances for errors. For example, in spam detection, false positives (legitimate emails marked as spam) may be more disruptive than false negatives (spam emails not detected). Thus, precision might be prioritized over recall.\n",
    "\n",
    "**How to Choose the Right Metric**\n",
    "**Understand the Problem Domain:**\n",
    "\n",
    "    Analyze the consequences of different types of errors (false positives and false negatives) in the context of the problem. Determine whether precision, recall, or a balance of both (F1 score) is more relevant.\n",
    "\n",
    "**Consider Class Distribution:**\n",
    "\n",
    "    Evaluate the class distribution in the dataset. For imbalanced datasets, metrics like Precision-Recall AUC or the F1 score can provide a better understanding of model performance than accuracy.\n",
    "\n",
    "**Use Multiple Metrics:**\n",
    "\n",
    "    Often, using a combination of metrics provides a more comprehensive view of model performance. For instance, reporting both precision and recall along with accuracy can give a clearer picture.\n",
    "\n",
    "**Application-Specific Metrics:**\n",
    "\n",
    "    In some cases, specific metrics like ROC-AUC (Area Under the Receiver Operating Characteristic Curve) are useful. ROC-AUC is particularly valuable for binary classification problems as it considers both true positive rate (sensitivity) and false positive rate.\n",
    "\n",
    "***Example Metrics and Their Use Cases***\n",
    "**Accuracy:**\n",
    "\n",
    "    Suitable when class distribution is balanced and all types of errors are equally important.\n",
    "\n",
    "**Precision:**\n",
    "\n",
    "    Important when the cost of false positives is high. For example, in email spam detection, marking a legitimate email as spam (false positive) should be minimized.\n",
    "\n",
    "**Recall (Sensitivity):**\n",
    "\n",
    "    Crucial when the cost of false negatives is high. For instance, in cancer detection, missing a diagnosis (false negative) is more critical than a false alarm.\n",
    "\n",
    "**F1 Score:**\n",
    "\n",
    "    Useful when there is a need to balance precision and recall, especially in cases of imbalanced class distribution.\n",
    "\n",
    "**ROC-AUC:**\n",
    "\n",
    "    Helpful in understanding the trade-off between true positive rate and false positive rate across different threshold settings.\n",
    "\n",
    "By carefully choosing and using appropriate evaluation metrics, one can ensure a more accurate and meaningful assessment of model performance, tailored to the specific needs and challenges of the classification problem at hand. This, in turn, helps in building more robust and effective models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54766f8",
   "metadata": {},
   "source": [
    "## Q8. Provide an example of a classification problem where precision is the most important metric, and explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6320dc",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b6203f",
   "metadata": {},
   "source": [
    "One example where precision is the most important metric is email spam detection. Let's break down why precision is crucial in this context.\n",
    "\n",
    "***Example: Email Spam Detection***\n",
    "**The Problem**\n",
    "\n",
    "    Classification Task: Identify whether an incoming email is spam or not spam.\n",
    "\n",
    "    Classes:\n",
    "\n",
    "        Positive Class: Spam\n",
    "\n",
    "        Negative Class: Not Spam (legitimate emails)\n",
    "\n",
    "**Why Precision is Important**\n",
    "Precision is defined as the proportion of true positive predictions (correctly identified spam emails) out of all positive predictions (emails identified as spam).\n",
    "\n",
    "$$ \\text{Precision} = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "**Consequences of False Positives**\n",
    "\n",
    "    User Experience: High precision is essential because false positives (legitimate emails incorrectly marked as spam) can severely impact user experience. Users might miss important emails, leading to frustration and potential loss of critical information.\n",
    "\n",
    "    Trust: If users lose trust in the spam filter due to frequent false positives, they might stop relying on it, defeating the purpose of having a spam filter in the first place.\n",
    "\n",
    "    Business Impact: In a business context, missing important emails could result in missed opportunities, loss of customer communications, and overall negative impact on productivity.\n",
    "    \n",
    "**Precision Over Other Metrics**\n",
    "\n",
    "    Recall (Sensitivity): While recall (the ability to identify all actual spam emails) is also important, it is secondary to precision in this scenario. Users might tolerate a few spam emails in their inbox (false negatives) more than missing important legitimate emails.\n",
    "\n",
    "    F1 Score: Balances precision and recall, but in this context, precision takes precedence. The cost of false positives is higher than the cost of false negatives.\n",
    "\n",
    "By focusing on precision, the spam filter ensures that when it marks an email as spam, it is very likely to be spam, thus maintaining user trust and reducing the likelihood of important emails being missed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a5583",
   "metadata": {},
   "source": [
    "## Q9. Provide an example of a classification problem where recall is the most important metric, and explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21de09fb",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d3f21",
   "metadata": {},
   "source": [
    "An example where recall is the most important metric is in medical diagnostics, specifically for a disease detection model. Let's dive into why recall is crucial in this context.\n",
    "\n",
    "***Example: Cancer Detection***\n",
    "**The Problem**\n",
    "\n",
    "    Classification Task: Identify whether a patient has cancer (positive class) or does not have cancer (negative class) based on medical tests and features.\n",
    "\n",
    "**Why Recall is Important**\n",
    "Recall, also known as sensitivity, is the proportion of true positive predictions (correctly identified cancer cases) out of all actual positive cases (all patients who have cancer).\n",
    "\n",
    "$$ \\text{Recall} = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "**Consequences of False Negatives**\n",
    "\n",
    "    Patient Health: High recall is essential because false negatives (patients incorrectly identified as not having cancer) can be life-threatening. Missing a cancer diagnosis means the patient does not receive the necessary treatment in time, which can lead to disease progression and potentially fatal outcomes.\n",
    "\n",
    "    Early Detection: In cancer detection, early diagnosis is often critical for successful treatment. Ensuring high recall means more cases of cancer are detected early, which can significantly improve treatment effectiveness and patient survival rates.\n",
    "\n",
    "    Minimizing Risk: False negatives pose a greater risk than false positives in this context. A false positive (incorrectly identified as having cancer) can lead to additional tests and temporary stress for the patient, but it is generally less harmful than missing an actual cancer case.\n",
    "\n",
    "**Recall Over Other Metrics**\n",
    "\n",
    "    Precision: While precision (the proportion of true positive predictions out of all positive predictions) is also important, recall takes precedence in this scenario. The cost of missing a diagnosis (false negative) is much higher than the cost of a false alarm (false positive).\n",
    "\n",
    "    F1 Score: Balances both precision and recall, but in this context, maximizing recall is more critical to ensure that as many cancer cases as possible are detected.\n",
    "\n",
    "By prioritizing recall, the model ensures that it captures as many true cases of cancer as possible, thereby minimizing the risk of missed diagnoses and improving patient outcomes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
